{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](images/dsep-banner.png)\n",
    "# **Welcome to the Applied Methods for Social Sciences in Python Workshop**\n",
    "\n",
    "By John, Barry, and Simran\n",
    "\n",
    "In Collaboration with the Division of Data Science's [Data Peer Consulting](https://data.berkeley.edu/ds-peer-consulting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEFORE WE BEGIN PLEASE COMPLETE THIS SURVEY\n",
    "[Pre-Workshop Survey Link](https://forms.gle/e1Dq8NcJF8xPye6N8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## John Park\n",
    "![John](https://data.berkeley.edu/sites/default/files/styles/width_400/public/john_pic_-_john_park_0.jpg?itok=-kg9pNQg&timestamp=1599267808)\n",
    "\n",
    "Quick Facts About Me:\n",
    "\n",
    "    üêª Senior at Cal\n",
    "    üéí Studying Computer Science and Economics\n",
    "    üè¢ Interned and returning full-time as an SDE@Amazon\n",
    "    üìä Joined the Data Peer Consulting team in Fall 2018\n",
    "\n",
    "How to Reach Me:\n",
    "\n",
    "    üìÆ Email: jhp@berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bharadwaj Swaminathan\n",
    "\n",
    "![Barry](images/barry.jpg)\n",
    "\n",
    "Quick Facts About Me:\n",
    "\n",
    "    üêª Senior at Cal\n",
    "    üéí Studying Data Science and Economics\n",
    "    üè¢ COVID-19 researcher at University of West Florida\n",
    "    üìä Joined the Data Peer Consulting team in Fall 2018\n",
    "\n",
    "How to Reach Me:\n",
    "\n",
    "    üìÆ Email: bharadwaj.s@berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simran Sachdev\n",
    "![Simran](https://data.berkeley.edu/sites/default/files/styles/width_400/public/headshot_-_simran_sachdev.jpg?itok=PgdDBm5M&timestamp=1599267430)\n",
    "\n",
    "Quick Facts About Me:\n",
    "\n",
    "    üêª Senior at Cal\n",
    "    üéí Studying Data Science and Applied Math\n",
    "    üè¢ Data Scientist Intern at Boston Scientific\n",
    "    üìä Joined the Data Peer Consulting team in Spring 2020\n",
    "\n",
    "How to Reach Me:\n",
    "\n",
    "    üìÆ Email: ssach@berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"tof\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "Use anchors to set these hyperlinks to jump to certain locations in the notebook. \n",
    "\n",
    "- [Introduction](#1)\n",
    "- [Example](#2)\n",
    "- [Reference Sheets](#rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workshop Goals\n",
    "\n",
    "The goal of this workshop is to cover the fundamental tools offered in Python for applied methods in the social sciences. \n",
    "\n",
    "    - Learn how to import and modify data in Python via Pandas\n",
    "    - Apply OLS and related statistical techniques\n",
    "    - Demonstrate a basic example workflow from raw data to analysis\n",
    "    \n",
    "Specifically, we will be working with `statsmodels`. This package provides a wide range of useful statistical tools for the social sciences, including but not limited to least squares, panel methods, mixed models, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Why Python?\n",
    "\n",
    "Python is one of the most popular general-purpose computing languages, and for good reason. Python's readability, maintainability, and robust community support, especially in the IPython sphere, makes it a strong choice for data science, including for the social sciences. \n",
    "\n",
    "### Transitioning from R\n",
    "\n",
    "`statsmodels` provides support for R-like formula expressions. You can read more about it [here](https://www.statsmodels.org/stable/examples/notebooks/generated/formulas.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Lets go through a basic example from the [statsmodels documentation](https://www.statsmodels.org/stable/gettingstarted.html). The dataset is a collection of historical data used in support of Andre-Michel Guerry‚Äôs 1833 Essay on the Moral Statistics of France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab sample data from Statsmodels dataset repo\n",
    "\n",
    "df = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note that the data is already in a Pandas DataFrame\n",
    "\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore and see if literacy rates are associated with per capita entries in the Royal Lottery. Our model will also need to control for wealth in each 'department'.\n",
    "\n",
    "We will use OLS to estimate this model, as described below:\n",
    "\n",
    "$$y = {\\beta}X + \\epsilon$$\n",
    "\n",
    "Where $y$ is a $N * 1$ column of per-capita lottery wagers (Lottery), while $X$ is an $N * 3$ matrix with a constant, literacy, and wealth.\n",
    "\n",
    "We know then that the OLS estimates on the coefficients will be:\n",
    "\n",
    "$$\\hat{\\beta} = (X'X)^{-1}X'y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually implement a model, there are a couple options, especially with regards to how we could encode dummy variables (if we had them). For this example, we'll be using Patsy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach Using Patsy\n",
    "\n",
    "\n",
    "Patsy provides a convenient way to encode design matrcies using R-like formulas. You can see the docs [here](https://patsy.readthedocs.io/en/latest/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices('Lottery ~ Literacy + Wealth', data=df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the resulting design matrix/data frame looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the resultant matrix/data\n",
    "\n",
    "- includes a constant to the exog regressors matrix (very useful)\n",
    "- are pandas dataframes\n",
    "\n",
    "Let's now fit the model! The workflow for statsmodels OLS is straightforward\n",
    "\n",
    "- 1. Define X, y (we just did this)\n",
    "- 2. Choose the appropriate statsmodels model class, and define a model (we'll use sm.OLS)\n",
    "- 3. Fit the model and inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define model\n",
    "\n",
    "model1 = sm.OLS(y, X)\n",
    "\n",
    "### Fit model\n",
    "\n",
    "results1 = model1.fit()\n",
    "\n",
    "### Inspect model\n",
    "\n",
    "print(results1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract raw result data via the results object attributes. You can read more [here](https://www.statsmodels.org/stable/regression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### all extractable attributes \n",
    "\n",
    "print(dir(results1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### grab coefficients\n",
    "\n",
    "results1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Or get rsquared\n",
    "\n",
    "results1.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model (basic edition)\n",
    "\n",
    "Statsmodels gives us a [range](https://www.statsmodels.org/stable/stats.html#residual-diagnostics-and-specification-tests) of diagnostic and specification tests. \n",
    "\n",
    "Let's try out the Rainbow test of linearity: the null is that the model is correct in being linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.linear_rainbow(results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first value we will get from the test is the F-stat while the second is the p-value. As our p-value is comfortably above our rejection region (standard is $\\alpha = 0.05$), we know that modelling this problem as linear is not incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn! (10 min)\n",
    "\n",
    "Let's try running this regression, but now controlling for regional heterogeniety. In other words, **include** *region* by creating dummy variables representing each region. $X$ should now be a $N * 7$.\n",
    "\n",
    "**Note:** Patsy handles this for you ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What should our fomula be?\n",
    "y, X = dmatrices('...', data=df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Define your model\n",
    "\n",
    "model = ...\n",
    "\n",
    "### Fit your model\n",
    "\n",
    "results = ...\n",
    "\n",
    "### Inspect your model\n",
    "\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your model\n",
    "\n",
    "print('Rainbow test pval: ' + str(sm.stats.linear_rainbow(results)[1]))100jjkjjj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do our results here compare to the model in which we controlled for region? Take a look at the rsquared, as well as the error bars on the coefficients. Discuss this within your group :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "##         Example: Contingent valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file <code>LoomistForestCVDataset.csv</code> contains a subset of the dataset used by Loomis et al. (1996). The dataset consists of five columns. The first column lists a bid amount randomly proposed to a respondent to assess their willingness to pay for a fire management program for old growth Pacific Northwest forests. The second column lists the number of respondents saying yes to the project under version one of the survey. The third column lists the number of respondents saying no to the project under version one of the survey. Columns four and five give yes and no responses for the same bid amount but under a different version of the survey. \n",
    "\n",
    "\n",
    "Credit to Prof. Graham's Ec141 course materials for this excercise. Check out his github [here](https://github.com/bryangraham)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loomisCVD = pd.read_csv(\"./data/LoomisCVData.csv\")\n",
    "loomisCVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lets begin by writing a short Python script to transform the given datafile into one respondent per row form. The first column should equal $D = 1$ if the respondent answered yes, $D = 0$ if they answered no. The second column should give the bid amount, $A$. The third column is $X = 1$ if the response was solicited from version one of the survey and $X = 0$ if it was from version two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    D_ = []\n",
    "    A_ = []\n",
    "    X_ = []\n",
    "    \n",
    "    def add_trow(d, a, x):\n",
    "        D_.append(d)\n",
    "        A_.append(a)\n",
    "        X_.append(x)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        b = row['BidAmount']\n",
    "        for i in range(row['NumYes_v1']):\n",
    "            add_trow(1,b,1)\n",
    "        for i in range(row['NumNo_v1']):\n",
    "            add_trow(0,b,1)\n",
    "        for i in range(row['NumYes_v2']):\n",
    "            add_trow(1,b,0)\n",
    "        for i in range(row['NumNo_v2']):\n",
    "            add_trow(0,b,0)\n",
    "            \n",
    "    return pd.DataFrame(columns=['D', 'A', 'X'], data=np.array([D_, A_, X_]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_transformed = transform_data(loomisCVD)\n",
    "\n",
    "### transformed data set\n",
    "print(cvd_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let's assume that willingness-to-pay for the fire management program for a randomly sampled person is: $$ W = \\alpha + X'\\beta + V$$ where $V | X, A ‚àº \\it  \\mathcal{N}(0, \\sigma^2)$ captures heterogeneity in willingness-to-pay across individuals. \n",
    "\n",
    "(Is this assumption robust? See errata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Assume that individuals respond yes to the proposal if their willingness-to-pay exceeds the bid they were offered. Then:\n",
    "\n",
    "$$Pr (D = 1| X, A) = \\Phi\\left(\\frac \\alpha \\sigma - \\frac 1 \\sigma A + X' \\frac \\beta \\sigma \\right)$$ with $\\Phi(*)$ the CDF of the standard normal distribution.\n",
    "\n",
    "(See errata for proof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Let's use probit regression analysis to construct estimates of the composite parameters $\\frac \\alpha \\sigma$, $- \\frac 1 \\sigma$, $\\frac \\beta \\sigma$. From these estimates we'll recover estimates of the fundamental preference parameters $\\alpha$, $\\beta$ and $\\sigma$. We'll implement a bootstrap procedure to construct standard error estimates for these parameters. \n",
    "\n",
    "Why/what does bootstrapping allow us to do? Why probit vs logit? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define endog, explanatory variables\n",
    "X = sm.add_constant(cvd_transformed[['A', 'X']])\n",
    "y = cvd_transformed['D']\n",
    "\n",
    "#probit model\n",
    "probit_model = sm.Probit(y, X)\n",
    "results = probit_model.fit()\n",
    "param_estimates = results.params\n",
    "print(param_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bootstrap Procedure: Probit on const, A, X\n",
    "\n",
    "B = 1000\n",
    "N = 260\n",
    "\n",
    "def param_bootstrap(df):\n",
    "    const = []\n",
    "    A_ = []\n",
    "    X_ = []\n",
    "    for i in range(B):\n",
    "        \n",
    "        sample_df = df.sample(n=N, replace=True)\n",
    "        \n",
    "        # probit procedure from earlier\n",
    "        X = sm.add_constant(sample_df[['A', 'X']])\n",
    "        y = sample_df['D']\n",
    "        \n",
    "        probit_model = sm.Probit(y, X)\n",
    "        results = probit_model.fit()\n",
    "        \n",
    "        const.append(results.params['const'])\n",
    "        A_.append(results.params['A'])\n",
    "        X_.append(results.params['X'])\n",
    "    return pd.DataFrame(columns=['alpha', 'sigma', 'beta'], data=np.array([const, A_, X_]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_param_results = param_bootstrap(cvd_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimated parameters\n",
    "estimate_sigma = -(1 / param_estimates['A'])\n",
    "estimate_beta = estimate_sigma * param_estimates['X']\n",
    "estimate_alpha = estimate_sigma * param_estimates['const']\n",
    "\n",
    "#estimated parameters from boot strap\n",
    "sigma_bs = -(1/bootstrap_param_results['sigma'])\n",
    "beta_bs = bootstrap_param_results['beta'] * sigma_bs\n",
    "alpha_bs = sigma_bs * bootstrap_param_results['alpha']\n",
    "\n",
    "bs_params=pd.DataFrame(columns=['alpha', 'sigma', 'beta'], data=np.array([alpha_bs, sigma_bs, beta_bs]).T)\n",
    "\n",
    "print(estimate_alpha,estimate_sigma, estimate_beta)\n",
    "print(bs_params.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our recovered parameters from our probit regression onto the dataset is:\n",
    "\n",
    "$$\\hat{\\alpha} = 89.93$$\n",
    "$$\\hat{\\sigma} = 104.96$$\n",
    "$$\\hat{\\beta} = -16.86$$\n",
    "\n",
    "Our bootstrap recovered params have the characteristics as shown in the table above, with recovered standard errors as follows:\n",
    "\n",
    "$$SE(\\hat{\\alpha})  = 13.22 $$\n",
    "$$SE(\\hat{\\sigma}) = 17.44 $$\n",
    "$$SE(\\hat{\\beta}) = 17.80 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "Our results for the parameters were as expected; the sign of the coefficient on beta is interesting however. Loomis, in the literature, found no statistical difference between the two surveys. Indeed, although our computed coefficient on beta is less than zero, it falls well within a $0.05$-significance around zero (which indicates that we also found no statistical difference between the surveys, as our confidence band would be $[-35.6, 35.6]$, which our computed beta falls within). Thus our results fall in line with what was expected both from the literature and intuitively (such as higher bid prices indicating lower probability of support). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are part of an environmental conservation group that is campaigning for a ballot initiative that would fund a fire management program like the one studied by Loomis et al. (1996). The type of initiative you wrote needs to pass with a majority of 67 percent. Your organization wrote the ballot initiative with a proposed tax of $\\hat{A}^{‚àó} ‚àí0.05$ per person, with $\\hat{A}^{‚àó}$ equal to $$ \\hat{A}^{‚àó} = \\hat{\\alpha} ‚àí \\hat{\\sigma}\\Phi^{-1}(0.67)$$ Here $\\hat{\\alpha}$ and $\\hat{\\sigma}$ correspond to your point estimates from question 4 above. Explain the\n",
    "reasoning behind choosing the proposed tax in this way? Construct an estimate of this\n",
    "tax (as well as a standard error using the bootstrap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "tax_estimate = estimate_alpha - estimate_sigma*norm.ppf(0.67) - 0.05\n",
    "print(tax_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bootstrap Procedure: Tax = A^* - 0.05*\n",
    "\n",
    "B = 10000\n",
    "N = 260\n",
    "def a_est_bootstrap(df):\n",
    "    \n",
    "    a_hats = []\n",
    "    \n",
    "    for i in range(B):\n",
    "        \n",
    "        #sample with replacement\n",
    "        sample_df = df.sample(n=N, replace=True)\n",
    "        \n",
    "        X = sm.add_constant(sample_df[['A', 'X']])\n",
    "        y = sample_df['D']\n",
    "        \n",
    "        probit_model = sm.Probit(y, X)\n",
    "        results = probit_model.fit()\n",
    "        \n",
    "        sigma_hat = -(1 / results.params['A'])\n",
    "        alpha_hat = sigma_hat * results.params['const']\n",
    "        \n",
    "        a_hat_estimate = alpha_hat - (sigma_hat*norm.ppf(0.67)) - 0.05\n",
    "        a_hats.append(a_hat_estimate)\n",
    "        \n",
    "    return pd.DataFrame(columns=['a_hat'], data=np.array([a_hats]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat_bs = a_est_bootstrap(cvd_transformed)\n",
    "a_hat_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_hat_bs.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the tax in this fashion in order to target the WTP of at least $67\\%$ of the population (i.e. to find the tax-level that is less than or equal to the WTP of at least $67\\%$ of the population). This is ensured by the $\\hat{\\sigma}\\Phi^{-1}(0.67)$ term, as it basically moves $\\hat{\\alpha}$ (the mean willingness to pay) by the amount needed to capture $67\\%$ of the sampled population willingness to pay. \n",
    "\n",
    "We find our estimate of the tax to be: $43.70$ and the standard error via bootstrap to be $12.81$ as shown in the result above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"rs\"></a>\n",
    "### Reference Sheets!\n",
    "[Back to Table of Contents](#tof)\n",
    "\n",
    "Links updated as of 1/1/11.\n",
    "\n",
    "- [NumPy Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)  \n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)  \n",
    "- [Matplotlib Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Matplotlib_Cheat_Sheet.pdf)  \n",
    "- [Seaborn Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Seaborn_Cheat_Sheet.pdf)\n",
    "\n",
    "Documentation\n",
    "\n",
    "- [Statsmodels Docs](https://www.statsmodels.org/stable/index.html)\n",
    "- [Linearmodels Docs](https://bashtage.github.io/linearmodels/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Resources\n",
    "1. Data Peer Consultants - That's us! We help undergrads and graduate students with projects, research, and more! Come to our drop-in hours.  \n",
    "https://data.berkeley.edu/ds-peer-consulting\n",
    "\n",
    "2. Towards Data Science - Website full of good blogs and helpful introductions to data science stuff.  \n",
    "https://towardsdatascience.com/\n",
    "\n",
    "3. Stack Overflow // Google - A great data scientist is adept at using StackOverflow and Google to find the answers to their bugs. More likely than not, someone out there has ran into the exact same problem as you, so might as well use their solutions as a resource!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks for Coming! PLEASE COMPLETE THIS POST-WORKSHOP SURVEY!  \n",
    "[Post-Workshop Survey](https://forms.gle/A7Dt5pADe57XdWBt8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errata\n",
    "\n",
    "#### 2. \n",
    "The survey design ensures independence of $V$ and $X$ and $A$ as $X$ and $A$ are randomly assigned to the population without any selection critereon. More specifically, the general random selection of surveyees coupled with the further random assignment of $X$ (via survey version) ensures independence by construction. \n",
    "\n",
    "#### 3.\n",
    "\n",
    "We begin our analysis by examining the point of indifference between $W$ and $A$:\n",
    "\n",
    "$$A = W$$\n",
    "\n",
    "Standardize\n",
    "\n",
    "$$\\frac A \\sigma = \\frac W \\sigma$$\n",
    "\n",
    "Expand $W$\n",
    "\n",
    "$$\\frac{A}{\\sigma} = \\frac{\\alpha}{\\sigma} + X^{'} \\frac{\\beta}{\\sigma} + \\frac{V}{\\sigma}$$\n",
    "\n",
    "Normalize to mean zero:\n",
    "\n",
    "$$\\frac{\\alpha}{\\sigma} - \\frac{1}{\\sigma} A + X^{'} \\frac{\\beta}{\\sigma} + \\frac{V}{\\sigma}$$\n",
    "\n",
    "Let the above expression be $Y$. We realize that:\n",
    "\n",
    "$$Pr(D=1|X,A) = Pr(Y > 0 | X, A)$$\n",
    "\n",
    "Substituting:\n",
    "\n",
    "$$Pr(D=1|X,A) = Pr\\left(\\frac{\\alpha}{\\sigma} - \\frac{1}{\\sigma} A + X^{'} \\frac{\\beta}{\\sigma} + \\frac{V}{\\sigma} > 0 | X, A \\right)$$\n",
    "\n",
    "\n",
    "$$ = Pr\\left(\\frac{\\alpha}{\\sigma} - \\frac{1}{\\sigma} A + X^{'} \\frac{\\beta}{\\sigma} + \\frac{V}{\\sigma}> 0 | X, A \\right)$$\n",
    "\n",
    "\n",
    "$$ = Pr\\left(\\frac{V}{\\sigma} > -\\frac{\\alpha}{\\sigma} + \\frac{1}{\\sigma} A - X^{'} \\frac{\\beta}{\\sigma}| X, A \\right)$$\n",
    "\n",
    "By symmetry of the normal distribution and as $V | X, A ‚àº \\it  \\mathcal{N}(0, \\sigma^2)$:\n",
    "\n",
    "$$ = Pr\\left(\\frac{V}{\\sigma} < \\frac{\\alpha}{\\sigma} - \\frac{1}{\\sigma} A + X^{'} \\frac{\\beta}{\\sigma}| X, A \\right)$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$ Pr(D=1|X,A) = \\Phi\\left(\\frac{\\alpha}{\\sigma} - \\frac{1}{\\sigma} A + X^{'} \\frac{\\beta}{\\sigma}\\right)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
